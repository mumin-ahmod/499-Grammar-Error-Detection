{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mumin-ahmod/499-Grammar-Error-Detection/blob/main/GEC_LANG8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "uUwKFf2Cp3e8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import os\n",
        "for dirname, _,filenames in os.walk('kaggle/input'):\n",
        "  for filename in filenames:\n",
        "    print(os.path.join(dirname, filename))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yL2vYviVqzb5",
        "outputId": "03f51bd3-765a-4e6f-a628-e3edbc042829"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: happytransformer in /usr/local/lib/python3.10/dist-packages (3.0.0)\n",
            "Requirement already satisfied: torch>=1.0 in /usr/local/lib/python3.10/dist-packages (from happytransformer) (2.4.1+cu121)\n",
            "Requirement already satisfied: tqdm>=4.43 in /usr/local/lib/python3.10/dist-packages (from happytransformer) (4.66.5)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.30.1 in /usr/local/lib/python3.10/dist-packages (from happytransformer) (4.44.2)\n",
            "Requirement already satisfied: datasets<3.0.0,>=2.13.1 in /usr/local/lib/python3.10/dist-packages (from happytransformer) (2.21.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from happytransformer) (0.2.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from happytransformer) (3.20.3)\n",
            "Requirement already satisfied: accelerate<1.0.0,>=0.20.1 in /usr/local/lib/python3.10/dist-packages (from happytransformer) (0.34.2)\n",
            "Requirement already satisfied: tokenizers<1.0.0,>=0.13.3 in /usr/local/lib/python3.10/dist-packages (from happytransformer) (0.19.1)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (from happytransformer) (0.18.3)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate<1.0.0,>=0.20.1->happytransformer) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate<1.0.0,>=0.20.1->happytransformer) (24.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate<1.0.0,>=0.20.1->happytransformer) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate<1.0.0,>=0.20.1->happytransformer) (6.0.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate<1.0.0,>=0.20.1->happytransformer) (0.24.7)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate<1.0.0,>=0.20.1->happytransformer) (0.4.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets<3.0.0,>=2.13.1->happytransformer) (3.16.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets<3.0.0,>=2.13.1->happytransformer) (16.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets<3.0.0,>=2.13.1->happytransformer) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets<3.0.0,>=2.13.1->happytransformer) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets<3.0.0,>=2.13.1->happytransformer) (2.32.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets<3.0.0,>=2.13.1->happytransformer) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets<3.0.0,>=2.13.1->happytransformer) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets<3.0.0,>=2.13.1->happytransformer) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets<3.0.0,>=2.13.1->happytransformer) (3.10.9)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0->happytransformer) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.0->happytransformer) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.0->happytransformer) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0->happytransformer) (3.1.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.30.1->happytransformer) (2024.9.11)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb->happytransformer) (8.1.7)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb->happytransformer) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb->happytransformer) (3.1.43)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb->happytransformer) (4.3.6)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb->happytransformer) (2.16.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb->happytransformer) (1.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb->happytransformer) (71.0.4)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb->happytransformer) (1.16.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<3.0.0,>=2.13.1->happytransformer) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<3.0.0,>=2.13.1->happytransformer) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<3.0.0,>=2.13.1->happytransformer) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<3.0.0,>=2.13.1->happytransformer) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<3.0.0,>=2.13.1->happytransformer) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<3.0.0,>=2.13.1->happytransformer) (1.13.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<3.0.0,>=2.13.1->happytransformer) (4.0.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb->happytransformer) (4.0.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets<3.0.0,>=2.13.1->happytransformer) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets<3.0.0,>=2.13.1->happytransformer) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets<3.0.0,>=2.13.1->happytransformer) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets<3.0.0,>=2.13.1->happytransformer) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.0->happytransformer) (2.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets<3.0.0,>=2.13.1->happytransformer) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets<3.0.0,>=2.13.1->happytransformer) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets<3.0.0,>=2.13.1->happytransformer) (2024.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.0->happytransformer) (1.3.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->happytransformer) (5.0.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install happytransformer\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import STOPWORDS, WordCloud\n",
        "from happytransformer import HappyTextToText, TTSettings, TTTrainArgs\n",
        "from transformers import T5Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8pzEv1qx7Yu-"
      },
      "outputs": [],
      "source": [
        "#CONVERSION OF M2 FORMAT TO TEXT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LyoRuxUc3aZ1"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    m2 = open(\"/content/lang8.train.auto.bea19.m2\").read().strip().split(\"\\n\\n\")\n",
        "    out = open(\"fce.train.gold.bea19.txt\", \"w\")\n",
        "\n",
        "    skip = {\"noop\", \"UNK\", \"Um\"}\n",
        "\n",
        "    for sent in m2:\n",
        "        sent = sent.split(\"\\n\")\n",
        "        cor_sent = sent[0].split()[1:]\n",
        "        edits = sent[1:]\n",
        "        offset = 0\n",
        "        for edit in edits:\n",
        "            edit = edit.split(\"|||\")\n",
        "            if len(edit) < 3:\n",
        "                continue  # Skip this edit if it doesn't have the expected format\n",
        "            if edit[1] in skip:\n",
        "                continue\n",
        "            coder = int(edit[-1])\n",
        "            if coder != 0:\n",
        "                continue\n",
        "            span = edit[0].split()[1:]\n",
        "            start = int(span[0])\n",
        "            end = int(span[1])\n",
        "            cor = edit[2].split()\n",
        "            cor_sent[start+offset:end+offset] = cor\n",
        "            offset = offset-(end-start)+len(cor)\n",
        "        out.write(\" \".join(cor_sent)+\"\\n\")\n",
        "\n",
        "main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nsn2wpb46OMi"
      },
      "outputs": [],
      "source": [
        "#PREPROCESSING INCORRECT SENTENCE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Owvu9E17hUm"
      },
      "outputs": [],
      "source": [
        "file1 = open(\"/content/lang8.train.auto.bea19.m2\", \"r\")\n",
        "s1 = file1.read()\n",
        "\n",
        "each_sent = s1.split(\"\\n\\n\")\n",
        "\n",
        "incorrect = []\n",
        "for i in range(len(each_sent)):\n",
        "  temp = each_sent[i].split(\"\\n\")\n",
        "  temp = temp[0]\n",
        "  temp = temp.split(\" \")\n",
        "  temp = temp[1:]\n",
        "  temp = ' '.join(temp)\n",
        "  incorrect.append(temp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8UYjDnHVTle"
      },
      "source": [
        "**PREPROCESSING INCORRECT SENTENCE**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oswYjsE7Vnln"
      },
      "outputs": [],
      "source": [
        "file1 = open(\"/content/lang8.train.auto.bea19.m2\", \"r\")\n",
        "s1 = file1.read()\n",
        "\n",
        "each_sent = s1.split(\"\\n\\n\")\n",
        "\n",
        "incorrect = []\n",
        "for i in range(len(each_sent)):\n",
        "  temp = each_sent[i].split(\"\\n\")\n",
        "  temp = temp[0]\n",
        "  temp = temp.split(\" \")\n",
        "  temp = temp[1:]\n",
        "  temp = ' '.join(temp)\n",
        "  incorrect.append(temp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4lzP8viWOUV"
      },
      "source": [
        "**PREPROCESSING CORRECT SENTENCE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EC1EXq94WL0K"
      },
      "outputs": [],
      "source": [
        "file2 = open(\"/content/fce.train.gold.bea19.txt\", \"r\")\n",
        "s2 = file2.read()\n",
        "correct = s2.split(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmJ-jY4cWm5t"
      },
      "source": [
        "**STORING CORRECT AND INCORRECT SENTENCE IN DATAFRAME**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LkzbcAE7Wtn_"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame()\n",
        "df[\"target\"] = correct\n",
        "df[\"input\"] = incorrect"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROZAZ6_MW4yy"
      },
      "source": [
        "**STORING IN CSV FORMAT**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERUWlWrBW8rM",
        "outputId": "16122fc4-71bb-4cd1-f1c6-3398a91ba23a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(498360, 2)\n"
          ]
        }
      ],
      "source": [
        "# df.to_csv(\"data.csv\", index=False)\n",
        "# index = []\n",
        "# for i in range(len(df.values)):\n",
        "#   if df.values[i][0] == df.values[i][1]:\n",
        "#     index.append(i)\n",
        "\n",
        "# df = df.drop(index)\n",
        "# df.shape\n",
        "\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "df.to_csv(\"data.csv\", index=False)\n",
        "\n",
        "# Drop the rows where the first and second columns are the same\n",
        "df = df[df.iloc[:, 0] != df.iloc[:, 1]]\n",
        "\n",
        "# Print the shape of the modified DataFrame\n",
        "print(df.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWJgHteGXfaP"
      },
      "source": [
        "**CHECKING NULL VALUES**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OpPYYbnFXiXo"
      },
      "outputs": [],
      "source": [
        "df.isnull().values.any()\n",
        "\n",
        "df.dropna(inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EE5zDadOXqG8"
      },
      "source": [
        "**CHECKING DUPLICATE VALUES**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m2OYXVQOXtMU"
      },
      "outputs": [],
      "source": [
        "df.duplicated().values.any()\n",
        "df.drop_duplicates(inplace=True)\n",
        "df.reset_index(inplace=True, drop=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ueJ4u52qZZHX"
      },
      "source": [
        "**TEXT CLEANING**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gt0xRYvQZc1u"
      },
      "outputs": [],
      "source": [
        "def clean(text):\n",
        "  text = re.sub('<.*>', '', text)\n",
        "  text = re.sub('\\(.*\\)', '',text)\n",
        "  text = re.sub('\\[.*]', '', text)\n",
        "  text = re.sub('{.*}', '', text)\n",
        "  text = re.sub(\"[-+@#^/|*(){}$~`<>=_]\",\"\",text)\n",
        "  text = text.replace(\"\\\\\",\"\")\n",
        "  text = re.sub(\"\\[\",\"\",text)\n",
        "  text = re.sub(\"\\]\",\"\",text)\n",
        "  text = re.sub(\"[0-9]\",\"\",text)\n",
        "  return text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qH4pLbOoaE1h"
      },
      "source": [
        "**LOWERCASE CONVERSION**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xego8UFVaHsz"
      },
      "outputs": [],
      "source": [
        "def word_cloud(df,col):\n",
        "  comment_words = ''\n",
        "  stopwords = set(STOPWORDS)\n",
        "\n",
        "  for val in df[col]:\n",
        "    val = str(val)\n",
        "    tokens = val.split()\n",
        "    for i in range(len(tokens)):\n",
        "      tokens[i] = tokens[i].lower()\n",
        "\n",
        "    comment_words += \" \".join(tokens)+\" \""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERONyGsfHo_E"
      },
      "source": [
        "**SPLIT DATA INTO TRAINING AND EVALUATION**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RqfFJnvyHtAF"
      },
      "outputs": [],
      "source": [
        "train_ratio = 0.8\n",
        "train_size = int(train_ratio * len(df))\n",
        "train_data = df[:train_size]\n",
        "eval_data = df[train_size:]\n",
        "\n",
        "train_data.to_csv(\"train.csv\", index=False)\n",
        "eval_data.to_csv(\"eval.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ii22v7MdIJ3C"
      },
      "source": [
        "**TRUNCATE AND LOAD TRAIN DATASET**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QI_X3aziIGAw"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"train.csv\")\n",
        "desired_length = 10000\n",
        "truncated_df = df[:desired_length]\n",
        "\n",
        "truncated_df.to_csv(\"t_train.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEKUlsmBKL9Y"
      },
      "source": [
        "**TOKENIZATION**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "8n5_i1AhIOP2",
        "outputId": "e63ff7db-7c50-4167-ffce-36334bec6677"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        }
      ],
      "source": [
        "tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
        "\n",
        "input_sentences = train_data[\"input\"].tolist()\n",
        "input_encodings = tokenizer(input_sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "target_sentences = train_data[\"target\"].tolist()\n",
        "target_encodings = tokenizer(target_sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "train_data[\"input_ids\"] = input_encodings.input_ids.tolist()\n",
        "train_data[\"attention_mask\"] = input_encodings.attention_mask.tolist()\n",
        "train_data[\"target_ids\"] = target_encodings.input_ids.tolist()\n",
        "train_data[\"target_attention_mask\"] = target_encodings.attention_mask.tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "velY_m1AOhZZ"
      },
      "source": [
        "**MODEL TRAINING**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "referenced_widgets": [
            "c1441a1109e348c3a31d579f239c9992",
            "f586dd119ffc45619f4126d60ad977f0",
            "c81cdec1afba4e60a4831e8407da5479",
            "bb5b801f43c2421bbc540ce984826b1b",
            "cd9f68bb00c54c2bb1dad6a4636d68be",
            "b321f97a3fa740e8a0c11d436fc4d8f0"
          ]
        },
        "id": "pi9R28gsKOuH",
        "outputId": "ee9662dd-08df-4ac1-a38f-a4299bee8ed8"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c1441a1109e348c3a31d579f239c9992",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f586dd119ffc45619f4126d60ad977f0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c81cdec1afba4e60a4831e8407da5479",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bb5b801f43c2421bbc540ce984826b1b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cd9f68bb00c54c2bb1dad6a4636d68be",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/9000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b321f97a3fa740e8a0c11d436fc4d8f0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='95' max='2250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  95/2250 04:39 < 1:47:46, 0.33 it/s, Epoch 0.04/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.025600</td>\n",
              "      <td>1.193385</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "happy_tt = HappyTextToText(\"T5\", \"t5-base\")\n",
        "\n",
        "args = TTTrainArgs(batch_size=4)\n",
        "happy_tt.train(\"t_train.csv\", args=args)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAgG3JNoO2dE"
      },
      "source": [
        "**BEAM SEARCH ALGOROTIHM**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "s4AWmxc_OjmH"
      },
      "outputs": [],
      "source": [
        "def generator_text(input_text, happy_tt, tokenizer, beam_settings):\n",
        "  input_text = \"grammar: \" + input_text\n",
        "  result = happy_tt.generate_text(input_text, args=beam_settings)\n",
        "  return result.text\n",
        "\n",
        "beam_settings = TTSettings(num_beams=5, min_length=1, max_length=20)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0n0zR8bQQBn_"
      },
      "source": [
        "**Example**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6wMaYa7O6dp"
      },
      "outputs": [],
      "source": [
        "example_1 = \"my email id is abc@gmail.com\"\n",
        "result_1 = generate_text(example_1, happy_tt, tokenizer, beam_settings)\n",
        "print(result_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rNJZck5eQSZv"
      },
      "outputs": [],
      "source": [
        "example_2 = \"I am enjoys, writings articles ons AI and I also enjoy writings articles on AI\"\n",
        "result_2 = generate_text(example_2, happy_tt, tokenizer, beam_settings)\n",
        "print(result_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NeYFX01AQo_i"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyP4E1UkyeYBCg6wQ4lB53MW",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}